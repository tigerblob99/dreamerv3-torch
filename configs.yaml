defaults:

  logdir: null
  traindir: null
  evaldir: null
  offline_traindir: ''
  offline_evaldir: ''
  seed: 0
  deterministic_run: False
  steps: 1e6
  parallel: False
  eval_every: 1e4
  eval_episode_num: 10
  log_every: 1e4
  reset_every: 0
  device: 'cuda:0'
  compile: True
  precision: 32
  debug: False
  video_pred_log: True

  # Environment
  task: 'dmc_walker_walk'
  size: [64, 64]
  envs: 1
  action_repeat: 2
  time_limit: 1000
  grayscale: False
  prefill: 2500
  reward_EMA: True

  # Model
  dyn_hidden: 512
  dyn_deter: 512
  dyn_stoch: 32
  dyn_discrete: 32
  dyn_rec_depth: 1
  dyn_mean_act: 'none'
  dyn_std_act: 'sigmoid2'
  dyn_min_std: 0.1
  grad_heads: ['decoder', 'reward', 'cont']
  units: 512
  act: 'SiLU'
  norm: True
  encoder:
    {mlp_keys: '$^', cnn_keys: 'image', act: 'SiLU', norm: True, cnn_depth: 32, kernel_size: 4, minres: 4, mlp_layers: 5, mlp_units: 1024, symlog_inputs: True}
  decoder:
    {mlp_keys: '$^', cnn_keys: 'image', act: 'SiLU', norm: True, cnn_depth: 32, kernel_size: 4, minres: 4, mlp_layers: 5, mlp_units: 1024, cnn_sigmoid: False, image_dist: mse, vector_dist: symlog_mse, outscale: 1.0}
  image_standardize: False
  image_standardize_dataset: False
  image_std_min: 1e-3
  image_crop_height: 0
  image_crop_width: 0
  image_crop_random: False
  actor:
    {layers: 2, dist: 'normal', entropy: 3e-4, unimix_ratio: 0.01, std: 'learned', min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 3e-5, eps: 1e-5, grad_clip: 100.0, outscale: 1.0}
  critic:
    {layers: 2, dist: 'symlog_disc', slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 3e-5, eps: 1e-5, grad_clip: 100.0, outscale: 0.0}
  reward_head:
    {layers: 2, dist: 'symlog_disc', loss_scale: 1.0, outscale: 0.0}
  cont_head:
    {layers: 2, loss_scale: 1.0, outscale: 1.0}
  dyn_scale: 0.5
  rep_scale: 0.1
  kl_free: 1.0
  weight_decay: 0.0
  unimix_ratio: 0.01
  initial: 'learned'

  # Training
  batch_size: 16
  batch_length: 64
  train_ratio: 512
  pretrain: 100
  model_lr: 1e-4
  opt_eps: 1e-8
  grad_clip: 1000
  dataset_size: 1000000
  opt: 'adam'

  # Behavior.
  discount: 0.997
  discount_lambda: 0.95
  imag_horizon: 15
  imag_gradient: 'dynamics'
  imag_gradient_mix: 0.0
  eval_state_mean: False

  # Exploration
  expl_behavior: 'greedy'
  expl_until: 0
  expl_extr_scale: 0.0
  expl_intr_scale: 1.0
  disag_target: 'stoch'
  disag_log: True
  disag_models: 10
  disag_offset: 1
  disag_layers: 4
  disag_units: 400
  disag_action_cond: False

dmc_proprio:
  steps: 5e5
  action_repeat: 2
  envs: 4
  train_ratio: 512
  video_pred_log: false
  encoder: {mlp_keys: '.*', cnn_keys: '$^'}
  decoder: {mlp_keys: '.*', cnn_keys: '$^'}

dmc_vision:
  steps: 1e6
  action_repeat: 2
  envs: 4
  train_ratio: 512
  video_pred_log: true
  encoder: {mlp_keys: '$^', cnn_keys: 'image'}
  decoder: {mlp_keys: '$^', cnn_keys: 'image'}

crafter:
  task: crafter_reward
  step: 1e6
  action_repeat: 1
  envs: 1
  train_ratio: 512
  video_pred_log: true
  dyn_hidden: 1024
  dyn_deter: 4096
  units: 1024
  encoder: {mlp_keys: '$^', cnn_keys: 'image', cnn_depth: 96, mlp_layers: 5, mlp_units: 1024}
  decoder: {mlp_keys: '$^', cnn_keys: 'image', cnn_depth: 96, mlp_layers: 5, mlp_units: 1024}
  actor: {layers: 5, dist: 'onehot', std: 'none'}
  value: {layers: 5}
  reward_head: {layers: 5}
  cont_head: {layers: 5}
  imag_gradient: 'reinforce'

atari100k:
  steps: 4e5
  envs: 1
  action_repeat: 4
  train_ratio: 1024
  video_pred_log: true
  eval_episode_num: 100
  actor: {dist: 'onehot', std: 'none'}
  imag_gradient: 'reinforce'
  stickey: False
  lives: unused
  noops: 30
  resize: opencv
  actions: needed
  time_limit: 108000

minecraft:
  task: minecraft_diamond
  step: 1e8
  parallel: True
  envs: 16
  # no eval
  eval_episode_num: 0
  eval_every: 1e4
  action_repeat: 1
  train_ratio: 16
  video_pred_log: true
  dyn_hidden: 1024
  dyn_deter: 4096
  units: 1024
  encoder: {mlp_keys: 'inventory|inventory_max|equipped|health|hunger|breath|obs_reward', cnn_keys: 'image', cnn_depth: 96, mlp_layers: 5, mlp_units: 1024}
  decoder: {mlp_keys: 'inventory|inventory_max|equipped|health|hunger|breath', cnn_keys: 'image', cnn_depth: 96, mlp_layers: 5, mlp_units: 1024}
  actor: {layers: 5, dist: 'onehot', std: 'none'}
  value: {layers: 5}
  reward_head: {layers: 5}
  cont_head: {layers: 5}
  imag_gradient: 'reinforce'
  break_speed: 100.0
  time_limit: 36000

memorymaze:
  steps: 1e8
  action_repeat: 2
  actor: {dist: 'onehot', std: 'none'}
  imag_gradient: 'reinforce'
  task: 'memorymaze_9x9'

robosuite_lift:
  task: 'robosuite_Lift'
  size: [84, 84]
  envs: 1
  action_repeat: 1
  parallel: False
  steps: 5e4
  prefill: 0
  train_ratio: 512
  video_pred_log: true
  offline_eval_batches: 5
  offline_eval_video_batches: 2
  robosuite_task_name: 'Lift'
  robosuite_robots: 'Panda'
  robosuite_controller: 'OSC_POSE'
  robosuite_camera: 'agentview'
  robosuite_reward_shaping: true
  robosuite_control_freq: 20
  robosuite_render_device: -1
  robosuite_add_joint_trig: true
  robosuite_lowdim_keys: ['robot0_joint_pos', 'robot0_joint_vel', 'robot0_gripper_qpos', 'robot0_gripper_qvel']

debug:
  debug: True
  pretrain: 1
  prefill: 1
  batch_size: 10
  batch_length: 20

robomimic:
  # Robomimic offline setting: single agent camera and physical proprio targets.
  # aux_ sin/cos provided as inputs only (no explicit next-state vector supervision stored).
  encoder: &robomimic_encoder
    {mlp_keys: '^(robot0_joint_pos|robot0_joint_vel|robot0_gripper_qpos|robot0_gripper_qvel|aux_.*)$', cnn_keys: 'image', act: 'SiLU', norm: True, cnn_depth: 32, kernel_size: 4, minres: 4, mlp_layers: 5, mlp_units: 1024, symlog_inputs: True}
  decoder:
    {mlp_keys: '^(robot0_joint_pos|robot0_joint_vel|robot0_gripper_qpos|robot0_gripper_qvel)$', cnn_keys: 'image', act: 'SiLU', norm: True, cnn_depth: 32, kernel_size: 4, minres: 4, mlp_layers: 5, mlp_units: 1024, cnn_sigmoid: False, image_dist: mse, vector_dist: symlog_mse, outscale: 1.0}

bc_defaults:
  logdir: logdir/BC_MLP/default
  offline_traindir: ''
  offline_evaldir: ''
  device: 'cuda:0'
  encoder: *robomimic_encoder
  # Explicit key ordering for encoder inputs (must match training data).
  # These lists define the exact concatenation order for MLP and CNN features.
  bc_mlp_keys_order:
    - robot0_joint_pos
    - robot0_joint_vel
    - robot0_gripper_qpos
    - robot0_gripper_qvel
    - aux_robot0_joint_pos_sin
    - aux_robot0_joint_pos_cos
  aux_key_map:
    aux_robot0_joint_pos_sin: robot0_joint_pos_sin
    aux_robot0_joint_pos_cos: robot0_joint_pos_cos
  bc_cnn_keys_order:
    - image
  bc_epochs: 20
  bc_batch_size: 256
  bc_lr: 1e-4
  bc_weight_decay: 0.0
  bc_eval_every: 50
  bc_eval_split: 0.1
  bc_grad_clip: 100.0
  bc_shuffle: True
  bc_loss: 'mse'
  bc_save_path: ''
  bc_crop_height: 78
  bc_crop_width: 78
  # Replay evaluation mode: 'env' for environment replay, 'npz' for NPZ-based evaluation
  replay_use_npz: False
  npz_evaldir: ''

bc_can_PH:
  logdir: logdir/BC_MLP/can_PH
  offline_traindir: datasets/robomimic_data_MV/can_PH_train
  offline_evaldir: datasets/robomimic_data_MV/can_PH_eval
  bc_epochs: 1000
  bc_batch_size: 256
  bc_lr: 5e-4
  bc_eval_split: 0.1
  bc_loss: 'mse'

bc_lift_PH:
  logdir: logdir/BC_MLP/lift_PH
  offline_traindir: datasets/robomimic_data_MV/lift_PH_train
  #offline_evaldir: datasets/robomimic_data_MV/lift_PH_eval
  bc_epochs: 1000
  bc_batch_size: 256
  bc_lr: 3e-4
  bc_eval_split: 0.2
  bc_loss: 'mse'

lift_env_eval:
  robosuite_task: Lift
  #robosuite_robots:
  #  - Panda
  #robosuite_controller: OSC_POSE
  #robosuite_reward_shaping: false
  #robosuite_control_freq: 20
  #has_renderer: false
  #has_offscreen_renderer: true
  #ignore_done: true
  #use_camera_obs: true
  #camera_depths: false
  #camera_heights: 84
  #camera_widths: 84
  #camera_obs_keys:
  #  - agentview_image
  #  - robot0_eye_in_hand_image
  #bc_cnn_keys_order:
  #  - image
  #bc_mlp_keys_order:
  #  - robot0_joint_pos
  #  - robot0_joint_vel
  #  - robot0_gripper_qpos
  #  - robot0_gripper_qvel
  #  - aux_robot0_joint_pos_sin
  #  - aux_robot0_joint_pos_cos
  controller_configs:
    type: BASIC
    body_parts:
      right:
        type: OSC_POSE
        input_max: 1
        input_min: -1
        output_max: [0.05, 0.05, 0.05, 0.5, 0.5, 0.5]
        output_min: [-0.05, -0.05, -0.05, -0.5, -0.5, -0.5]
        kp: 150
        damping: 1
        impedance_mode: fixed
        kp_limits: [0, 300]
        damping_limits: [0, 10]
        position_limits: null
        orientation_limits: null
        uncouple_pos_ori: true
        control_delta: true
        interpolation: null
        ramp_ratio: 0.2
        input_ref_frame: world
        gripper:
          type: GRIP

can_env_eval:
  robosuite_task: PickPlaceCan
  #robosuite_robots:
  #  - Panda
  #robosuite_controller: OSC_POSE
  #robosuite_reward_shaping: false
  #robosuite_control_freq: 20
  #has_renderer: false
  #has_offscreen_renderer: true
  #ignore_done: true
  #use_camera_obs: true
  #camera_depths: false
  #camera_heights: 84
  #camera_widths: 84
  #camera_obs_keys:
  #  - agentview_image
  #  - robot0_eye_in_hand_image
  #bc_cnn_keys_order:
  #  - image
  #bc_mlp_keys_order:
  #  - robot0_joint_pos
  #  - robot0_joint_vel
  #  - robot0_gripper_qpos
  #  - robot0_gripper_qvel
  #  - aux_robot0_joint_pos_sin
  #  - aux_robot0_joint_pos_cos
  controller_configs:
    type: BASIC
    body_parts:
      right:
        type: OSC_POSE
        input_max: 1
        input_min: -1
        output_max: [0.05, 0.05, 0.05, 0.5, 0.5, 0.5]
        output_min: [-0.05, -0.05, -0.05, -0.5, -0.5, -0.5]
        kp: 150
        damping: 1
        impedance_mode: fixed
        kp_limits: [0, 300]
        damping_limits: [0, 10]
        position_limits: null
        orientation_limits: null
        uncouple_pos_ori: true
        control_delta: true
        interpolation: null
        ramp_ratio: 0.2
        input_ref_frame: world
        gripper:
          type: GRIP

joint_train:
  # --- model definition ---
  encoder: *robomimic_encoder
  mlp_layers: 3          # Number of MLP layers for policy and value networks
  act: 'SiLU'              # Activation function
  norm: True               # Use layer normalization

  # --- Training Loop ---
  steps: 50000             # Total training steps
  model_lr: 5e-4            # Learning rate for joint optimizer
  weight_decay: 1e-5        # Weight decay for joint optimizer

  batch_size: 64            # Batch size
  batch_length: 64          # Sequence length
  grad_clip: 100.0          # Gradient clipping norm
  log_every: 100          # Logging frequency (steps)
  save_every: 10000
  
  # --- Loss Scales ---
  wm_loss_scale: 0.01        # Scale for World Model loss (Recon + KL + Reward)
  bc_loss_scale: 1000.0        # Scale for Behavior Cloning loss
  
  # --- Data Augmentation (Dual Cropping) ---
  image_crop_height: 84     # Target crop height (from 84x84 source)
  image_crop_width: 84      # Target crop width
  wm_random_crop: False      # Random crop for World Model (Reconstruction)
  bc_random_crop: False      # Random crop for Policy (BC) - Independent of WM crop
  
  # --- Evaluation Settings ---
  eval_every: 1000         # Frequency of evaluation (steps)
  offline_eval_batches: 64  # Number of batches to calculate validation loss on
  eval_episodes: 50          # Number of online episodes to run in Robosuite
  max_env_steps: 500        # Max steps per online episode
  num_envs: 25
  robosuite_task: 'PickPlaceCan'  # Robosuite task name
  
  # --- Observation Handling (Robosuite Defaults) ---
  # These ensure the environment matches the encoder input structure
  camera_obs_keys: [agentview_image, robot0_eye_in_hand_image]
  flip_camera_keys: [agentview_image, robot0_eye_in_hand_image]
  bc_cnn_keys_order: [image]
  bc_mlp_keys_order: 
    - robot0_joint_pos
    - robot0_joint_vel
    - robot0_gripper_qpos
    - robot0_gripper_qvel
    - aux_robot0_joint_pos_sin
    - aux_robot0_joint_pos_cos
  aux_key_map:
    aux_robot0_joint_pos_sin: robot0_joint_pos_sin
    aux_robot0_joint_pos_cos: robot0_joint_pos_cos
    
  # --- System ---
  num_workers: 0            # Keep at 0 for efficient in-memory dataset usage
  video_pred_log: False      # Log reconstruction and rollout videos to WandB